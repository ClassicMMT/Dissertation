
% --------------------------------------------------------------------------------------
%                   LATEX TEMPLATE FOR DISSERTATION (HONS)
% --------------------------------------------------------------------------------------
\documentclass[11pt]{book}

\usepackage{amsfonts, amsmath, amssymb}  
\usepackage{times}
%\usepackage{hyperref}
\usepackage{wrapfig}
\usepackage[numbers]{natbib}
\usepackage{graphicx}

%\usepackage[backref=page,pagebackref=true,linkcolor = blue,citecolor = red]{hyperref}
%\usepackage[backref=page]{backref}



\DeclareGraphicsExtensions{.pdf,.png,.jpg}

\DeclareMathOperator*{\argmax}{arg\,max}


\setlength{\oddsidemargin}{1.5cm}
\setlength{\evensidemargin}{0cm}
\setlength{\topmargin}{1mm}
\setlength{\headheight}{1.36cm}
\setlength{\headsep}{1.00cm}
%\setlength{\textheight}{20.84cm}
\setlength{\textheight}{19cm}
\setlength{\textwidth}{14.5cm}
\setlength{\marginparsep}{1mm}
\setlength{\marginparwidth}{3cm}
\setlength{\footskip}{2.36cm}


\begin{document}
\pagestyle{empty}

%: ----------------------------------------------------------------------
%:                  TITLE PAGE: name, degree,..
% ----------------------------------------------------------------------


\begin{center}

\vspace{1cm}

%%% Type the thesis title below%%%%%%%%%%%%%%%%
{\Huge         Exploration of Adversarial Sub-Spaces}

\vspace{35mm} 

\includegraphics[width=2cm]{images/logo.jpg}

 \vspace{45mm}

%%%%%Type Your Name Below%%%%%%%%%%%%
{\Large       Peter Orlovskiy}

	\vspace{1ex}

School of Computer Science

The University of Auckland

	\vspace{5ex}

 %%%%%Typing Your Supervisors Name Below%%%%%%%%%%%%
Supervisor:             Joerg Wicker

	\vspace{30mm}

A dissertation  submitted in partial fulfillment of the requirements for the degree of Master of Data Science, The University of Auckland, 2025.

\end{center}


\newpage



%: --------------------------------------------------------------
%:                  FRONT MATTER:  abstract,..
% --------------------------------------------------------------
\chapter*{Abstract}       
\setcounter{page}{1}
\pagestyle{headings}
% \pagenumbering{roman}

\addcontentsline{toc}{chapter}{Abstract}






%: --------------------------------------------------------------
%:                  END:  abstract
% --------------------------------------------------------------



%: ----------------------- contents ------------------------
\setcounter{secnumdepth}{3} % organisational level that receives a numbers
\setcounter{tocdepth}{3}    % print table of contents for level 3
\tableofcontents            % print the table of contents
% levels are: 0 - chapter, 1 - section, 2 - subsection, 3 - subsection



%: --------------------------------------------------------------
%:                  MAIN DOCUMENT SECTION
% --------------------------------------------------------------
	
\chapter{Introduction}

Uncertainty quantification in model development, whether it is a statistical or a machine learning model, is critical, since the data the model is developed on is always limited. This means that the the decision function a model learns is based on incomplete information and therefore it is crucial to be able to quantify and understand the uncertainty surrounding a prediction. For example, in statistical models, it is common to report point estimates with corresponding prediction bounds (with an associated confidence score; i.e., 95\%), rather than the point estimates alone \cite{ELKINS2024319}.

For supervised classification tasks, machine learning models typically output a probability distribution over the classes, which can then be used to model the uncertainty in predictions. For example, in a cats vs. dogs prediction task, suppose we have two examples. On the first one, the model predicts the image to contain a cat with a 90\% probability and a dog with 10\% probability. For the second example, suppose this is 55\% and 45\%. For both examples, the model would predicts cat since the highest probability is associated with the cat label. However, it is obvious that in the first example the model is significantly more confident in its prediction than in the second. 

The problems arise because of the temptation to call the normalised model outputs a probability distribution, because even though some models such as logistic regression, naive Bayes etc. are probabilistic by design, neural networks are just optimising a score function and do not inherently try to learn a probability distribution. This means the ``probabilities'' one obtains from these models is not guaranteed to reflect true uncertainty and, in fact, the models are almost always overconfident \cite{}. This is because neural networks, by optimising the cross-entropy loss, encourage assigning higher values to the correct class, but normalising and learning anything about the distribution of the labels is not part of the process. Thus, being able to quantify uncertainty becomes a fundamental issue in model development since it allows model developers the ability to understand the reliability of their predictions and to determine whether the outputs the model is producing may not be applicable, i.e., the model is operating outside of the domain it has been trained on. 

For example, if a model has been trained to classify images of cats and dogs, then given an image of a shark, the model will still predict a cat or a dog although, obviously, this is incorrect. Likewise, if a self-driving car model was trained on videos during sunny days and clear nights, it may not respond as well to rarer conditions such as fog or smog. In these situations, it is important that a machine learning model is able to signal that it is unable to make an accurate prediction although, most of the time, the models are not even designed for this to be possible. This is called the problem of selective classification which directly ties into model confidence and uncertainty, since if the test data falls outside of the domain it was trained on, or if the prediction reliability is low, or there is difficulty in deciding on the prediction, the model should be able to abstain from making the prediction. 

However, this is easier said than done, since adding an abstaining mechanism (such as an "I don't know" class) complicates the model further, requires the model to be retrained, and comes with its own questions regarding the uncertainty pertaining to the new architecture. This means it is important to develop mechanisms where pre-trained networks are still able to abstain from making predictions without fundamental changes.


Another problem is that machine learning models are often highly confident in their predictions and are generally unable to abstain from making a prediction; most of the time they are not even designed for this to be possible.








A huge amount of models are now being created across all fields such as eCommerce, agriculture, aquaculture, weather modelling, government, etc., and these models help automate a large number of previously tedious or intractable problems. However, as they say, with great power comes great responsibility for not just performant, but also reliable, robust, and trustworthy models. This is especially true in critical fields such as healthcare, finance, or self-driving vehicles, where the impact of incorrect automated decision making can be catastrophic and potentially result in the loss of human life. 

Developing safe models is easier said than done, however, since \cite{szegedy2014intriguingpropertiesneuralnetworks} showed that machine learning models are susceptible to adversarial attacks - visually-identical constructed inputs on which models mis-perform; three examples are shown in Fig. \ref{fig:adversarial_attacks}. Although these attacks are constructed by researchers with the intent to fool the models, it is a crucial security risk, nevermind that it is possible that such adversarial examples exist in the ``wild''. For example, previous research found that stop signs can be categorised as stop signs if real (small) stickers are placed on them \cite{stopSignMisclassified}. However, stickers are not always stuck on stop signs by researchers, but possibly by individuals for their own entertainment. This is why it is crucial for machine learning practitioners to understand how their models work, when their models work, and why their models work and, perhaps most importantly, how, when, and why their models do \textit{not} work.

\begin{wrapfigure}{r}{0.5\textwidth}
 \vspace{-0pt}
 \includegraphics[scale=0.5]{images/stop_sign.png}
 \centering
 \caption{Incorrectly classified stop sign with high-confidence. Adapted from \cite{stopSignMisclassified}}
 \label{fig:stop}
 \vspace{-10pt}
\end{wrapfigure}

% On a high level, machine learning falls into three main buckets - supervised learning, unsupervised learning, and reinforcement learning. In short, supervised learning has inputs and targets, unsupervised learning has just inputs (with no targets), and reinforcement learning learns by interacting with an environment, receiving feedback (rewards/penalties) and trying to maximise the rewards. Supervised learning can be further split into regression and classification. Regression is where the targets are numeric/continuous and classification is where the targets are class labels; for example, dog or cat. This paper focuses on supervised classification since a ``successful'' adversarial example leads to misclassification but with regression it is more ambiguous. 











\section{Uncertainty Quantification}

The point of (supervised) machine learning, in practice, is to be able to develop a model which ``learns'' a decision function. This model can then be given inputs and, through the learnt decision function, the model is able to make a prediction about what the corresponding labels should be. 

However, evaluating the quality of the predictions in reality can be difficult since one often does not have the corresponding targets for the test set; if targets were always available, we would not need machine learning models. Therefore, being able to quantify uncertainty becomes essential, as it provides model practitioners the ability to understand the reliability of their predictions and to determine whether the outputs that the model is producing may not be applicable, i.e., the model is operating outside of the domain it has been trained on. 

For example, if a model has been trained to classify images of cats and dogs, then given an image of a shark, the model will still predict a cat or a dog although, obviously, this is incorrect. Likewise, if a self-driving car model was trained on videos during sunny days and clear nights, it may not respond as well to rarer conditions such as fog or smog. Another problem is that machine learning models are often highly confident in their predictions and are generally unable to abstain from making a prediction; most of the time they are not even designed for this to be possible.
%Finally, models' predicted probabilities are also typically not well-calibrated, meaning that the predicted probabilities do not reflect the true likelihood of outcomes. 

Because of this, whenever a machine learning model makes a prediction, it is important to critically assess its reliability. Important questions include: does the model have sufficient capacity to make this prediction? Is the prediction within the domain of data the model has seen, or is it an out-of-distribution case where the model may be prone to error? How reliable is the prediction, and what metrics or uncertainty estimates can help quantify this reliability? Finally, is the modelâ€™s reported confidence well-calibrated, or is it overconfident in cases where uncertainty is actually high?

Answering these questions is much more difficult and therefore it is important to be able to quantify a model's uncertainty in its predictions, as this can serve as another tool informing the practitioner of both the quality of the model and the model's outputs. 


In summary, given that a model has predicted a label, it is important to assess whether this prediction is applicable, reliable, and decideable. 











While these problems are significant, progress has been made towards addressing these shortcomings.

There is a significant body of literature examining how one can make a machine learning model reject making a prediction \cite{rejection_survey, geifman2017selective} when the model is not confident enough to make it or it falls outside of its applicability domain.

work has been done on how one can calibrate the confidence corresponding to predictions \cite{confidence_calibration, silva2023classifier}.

In fact, significant work has been done for both 

Even an untrained machine learning model will always generate some sort of prediction.


\chapter{Background}

\section{Formal Definitions}

Formally, for features $\mathcal{X}$ and labels $\mathbf{y}$, there is an input/feature vector $\mathcal{X}_i \in \mathbb{R}^k$, where $k$ is the dimensionality (i.e. number of pixels in an image), for which there is a corresponding label $y_i$ for $i\in 1,..,n$, where $n$ is the number of examples in the dataset. A machine learning model, $f$, outputs logits $f(X_i) = \mathbf{z}$, trying to minimise the loss $\ell(\mathbf{z}, y_i)$. Note that the dimensionality of the logit vector $\mathbf{z}$ is $C$, the number of classes. The logits $\mathbf{z}$ can be converted to predicted probabilities using the softmax function:

\begin{equation}
\label{eq:softmax}
    \text{Softmax}(z_i)  = p_i =
    \frac{e^{z_i}}{\sum_{j=1}^{C} e^{z_j}}, 
    \quad i = 1, \dots, C
\end{equation}

\noindent Applying softmax to all elements of $\mathbf{z}$ produces the predicted probabilities vector $\mathbf{p}$. Perhaps more commonly, the index of the maximum value in each row is taken as the predicted class.

A common loss function to use is the cross-entropy loss which is defined as:

\begin{equation}
\label{eq:crossentropyloss}
    \text{Cross Entropy} = -\sum_{i=1}^C \mathbf{1}\{i=y\} \log(p_i)
\end{equation}

\noindent where $\mathbf{1}\{\cdot\}$ is the indicator function, $y$ is the true label for the example, and $p_i$ is the predicted probabilities. Note that this is the general, multi-class no-reduction cross-entropy loss and, since there is no reduction, this loss outputs a value for every example. In practice during model training, there are multiple examples (the size of the first dimension of a batch) and the ``mean'' reduction is used, where Eq. \ref{eq:crossentropyloss} is calculated for all examples in the batch and then averaged. This ``loss'' is then minimised by the chosen optimizer. To make a prediction $\hat{y}_i=f(x_i)$, typically the index of the maximum value is taken as the predicted class. Note that since the logit to probability mapping (Eq. \ref{eq:softmax}) is order-preserving, there is no difference between whether the maximum value is found from the logits or the predicted probabilities.


\section{Uncertainty}

Types: Aleatoric vs. epistemic
Common measures: entropy, information gain, confidence scores, bayesian methods

There are two main types of uncertainty, aleatoric and epistemic and they 

The predicted probabilities $p_i$ can be thought of as a joint probability distribution across the classes which represents a model's certainty about its predictions. For example if class 2 has the highest predicted probability, then the model believes that, according to its learnt decision function, that class 2 is the most likely class for this example. However, there are many ways of quantifying the uncertainty from these predicted probabilities.

\subsection{Entropy}
\label{sec:background_uncertainty_entropy}

Entropy measures the average ``surprise'' in the predicted distribution. Formally, entropy is defined as:

\begin{equation}
\label{eq:entropy}
    \text{Entropy} = -\sum_{i=1}^C p_i \log (p_i)
\end{equation}

\noindent where $C$ is the number of classes and $p_i = \text{Softmax}(z_i)$ is the predicted probability of class $i$, where $z_i$ are the logits. Note that like \ref{eq:crossentropyloss}, this formulation is for a single example - there is no reduction.

\begin{wrapfigure}{r}{0.5\textwidth}
    \vspace{-20pt}
    \centering
    \includegraphics[width=0.5\textwidth]{images/chessboard_four_classes.png}
    \caption{Chessboard dataset with four classes.}
    \label{fig:chessboard_four_classes_boundary}
    \vspace{-10pt}
\end{wrapfigure}

Entropy values are high when the distribution is flat ($p_i\approx p_j, \forall i,j$) and low when there is a single class which has a much higher predicted probability than the rest. This directly corresponds to a model's uncertainty since a model is uncertain when all $p_i$ are similar (the model has difficulty choosing between one or thinks that multiple classes are likely), and certain when one probability is much higher than the rest (indicating no indecision between classes).

Consider the simulated four-class dataset shown in Figure \ref{fig:chessboard_four_classes_boundary}. A model was trained on this dataset and the decision boundaries are shown. Since there are four classes, there are four class regions and five distinct decision boundaries with two points where three boundaries meet. A point on a decision boundary separating two classes will have two predicted probabilities that are equal. For example, a point on the yellow/red boundary will have $p_{yellow}=p_{red}$ and similar for the other boundaries. 

For the areas where three boundaries meet, say yellow/red/blue, there will be $p_{yellow}=p_{red}=p_{dark\ blue}$. This means that Eq. \ref{eq:entropy} will be higher at that point and the red/dark blue/light blue points than anywhere else since these points, and their surrounding regions, represent the highest uncertainty where the model is unable to differentiate between any of these classes. On the flip side, the clusters inside each region will have a high predicted probability for the corresponding class and low for probabilities for the others, representing areas of low entropy.

Therefore, entropy is able to quantify a model's uncertainty with high values corresponding to high uncertainty. The advantage of using the entropy defined in Eq. \ref{eq:entropy} is that we do not need the true class labels; entropy can be computed just from the logits $z_i$. This is convenient for test sets for which no true class labels exist and one is only able to calculate the entropy.



\subsection{Information Content}
\label{sec:background_uncertainty_info}

Another information-theory approach to quantifying uncertainty is the self-information (also known as the surprise or the pointwise information content), of the model's predicted class; referred to as the information content for simplicity from now on. Given the predicted probabilities, $\mathbf{p}$, the information content is defined as,

\begin{equation}
\label{eq:information_content}
    \text{Information Content} = -\log(\max_i p_i)
\end{equation}

\noindent and shows how surprised a model is in its own prediction. If the predicted class probability is high, $\max_i p_i \approx1$, then the information content is close to 0, because the model is very confident in its prediction. However, if the predicted probability is very low, $\max_i p_i \approx 0$, then the information content will be a large number, indicating that the model is uncertain. Realistically, $\max_i p_i$ is minimised when all classes have the same predicted probability, i.e., $p_i = \frac{1}{C} , \forall i\in C$, i.e., when the point is at the intersection of all decision boundaries, or on the boundary if $C=2$.

However, because Eq. \ref{eq:information_content} only considers the maximum class, it is unable to differentiate between when the model is on the decision boundary or not. This will be discussed further in section \ref{sec:subtleties}. Note that since the information content approach only relies on the model's maximum probability, like entropy, it does not require the target labels for computation.



\subsection{Probability Gaps}
\label{sec:background_uncertainty_gaps}

Another alternative to entropy and information gain, is the logit or probability gaps. Logit gaps are the difference between the largest and second-largest logit \cite{logit_dist_adv_trained_dnn}, $z_1-z_2$, where $z_1$ is the largest logit and $z_2$ is the second-largest logit. Similarly, probability gaps are the difference between the largest and second largest predicted probability, $p_1 - p_2$. Finally, there are the normalised probability gaps considered in this work:

\begin{equation}
    \text{Probability gap} = \frac{p_1-p_2}{p_1}
\end{equation}

\noindent where $p_1$ and $p_2$ are the highest and second-highest predicted probabilities, which ensure that the probability gap is bounded: $\text{gap}\in[0,1]$. If the difference between the two highest probabilities is low, then the probability gap is low, indicating proximity to the decision boundary. The point where the the gap is minimised is when a point is on a decision boundary and the two highest probabilities are equal $p_1=p_2$. On the flip side, when the probability gap is large, the largest probability is much larger than all others, indicating that the example is inside confident regions because the model is confident in its classification.

Considering the example in Fig. \ref{fig:chessboard_four_classes_boundary}, probability gaps are ideal for determining proximity to decision boundaries but, unlike entropy, probability gaps are unable to distinguish between a point on the yellow/red boundary if a point is at the intersection of the red/yellow/dark blue boundaries.


\subsection{Conformal Prediction - should really be about selective classification}
\label{sec:background_uncertainty_conformal}

Another way of quantifying uncertainty can be through conformal prediction \cite{conformal_prediction_original_ref}, which is a rigorous form of uncertainty quantification that provides a statistically-guaranteed procedure for finding reliable measures for predictions made by machine learning models. Conformal prediction does not rely on any model or dataset, meaning that it is model and dataset agnostic, allowing it to be used for almost any problem with minimal restrictions. 

The purpose of conformal prediction is, typically, to generate prediction sets which have the guarantee that the probability of the true label being within these prediction sets is bounded below and above; given a pre-specified confidence level (i.e., 95\%) , the confidence level is used to calculate bounds within which this probability falls. This bounding property is called the coverage. However, as illustrated in \cite{conformal_prediction_tutorial_angelopoulos}, conformal prediction can also be used for selective classification with the following procedure.

Given a trained classifier $f$, and test features and labels, this procedure seeks to satisfy the following:

\begin{equation}
    \mathbb{P}(y_{test} = \hat{y}_{test} \ |\ \max_i p_i \ge \hat{\lambda}) \ge 1-\alpha
\end{equation}

\noindent where $\hat{y}_{test}=f(x_{test})$, $\max_i p_i$ is the model's highest predicted probability, and $\hat{\lambda}$ is a cut off chosen to guarantee that the upper-bound risk is guaranteed to be less than $\alpha$. 

NOT FINISHED.




\section{Applicability Domain}
\label{sec:applicability_domain}

* Origin in cheminformatics/QSAR
* Extension to ML models

The applicability domain defines the regions where the chosen model evaluation metric is trustworthy. For example, within the valid range of the data, them metric may be trustworthy but distance metrics outside this range may no longer be trustworthy.




\section{Adversarial Learning}

\subsection{Adversarial Attacks}

In general, an adversarial example $\mathbf{x'}=\mathbf{x}+\boldsymbol{\eta}$ is an altered version of an input $\mathbf{x}$, with adversarial noise $\boldsymbol{\eta}$, generated by an adversarial attack $g$, given to a machine learning model $f$, which causes the model to perform worse. How the model "performs worse" depends on the task; typically, adversarial literature has focused on classification and while it is possible (and sometimes trivial) to adapt these adversarial attacks for regression tasks, the results are typically less interesting. This is because in classification, an adversarial example is successful if $f(\mathbf{x}+\boldsymbol{\eta}) \ne f(\mathbf{x}) = y$, where $y$ is the true label. However, in regression tasks, $y$ is a continuous value, so we may have higher mean squared error (MSE) with the adversarial example, but it's, perhaps, less obvious that the performance is much worse.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{images/adversarial_attacks.png}
    \caption{Adversarial examples on the MNIST dataset. The top row shows the original examples, the titles contain the original labels, the second row shows the (exaggerated) noise, $\eta$, and the third row shows the adversarial examples. Note that the model originally classified these original examples correctly}
    \label{fig:adversarial_attacks}
\end{figure}

Typically, adversarial examples can be categorised into two main buckets, poisoning and evasion \cite{wild_patterns}. The former "poisons" the training data to compromise normal model function. This means that given unaltered, normal examples at test time, the model will either perform worse, have an implanted back-doors or trojans etc. The importance of mitigating such attacks can not be understated. On the other hand, evasion attacks, do not compromise normal model function, but focus on generating adversarial examples which make a classifier misclassify. These can be anything from specially crafted noise \cite{carlini_wagner, goodfellow2015} to adversarial patches \cite{adversarial_patches} and can be very effective.

Adversarial attacks can also be targeted (see \cite{carlini_wagner}) or untargeted (see \cite{goodfellow2015}). An untargeted attack is one such that $f(\mathbf{x})=y \ne f(\mathbf{x}+\boldsymbol{\eta})$, and a targeted attack is when we have a target class $t$ and an example is adversarial if $f(\mathbf{x})=y$ and $f(\mathbf{x} +\boldsymbol{\eta})=t$. This work focuses on the latter since we are not interested in the attacks themselves here, but are more interested in the regions that these attacks are found.

Trivial examples of inputs causing a model to misclassify are not hard to come up with. For example, if $f(\mathbf{x})$ outputs "dog", then creating $\mathbf{x'} = \mathbf{0}$, i.e. setting all pixels to black, will make any classifier "misclassify". However, examples like this are not interesting because even a human would then "misclassify" them. Thus, the examples we are actually interested in are those where a human would have no problem making a correct classification. Because of this, adversarial literature has typically focused on adversarial attacks imperceptible to the human eye. 

Formally, we define an adversarial attack as a procedure (could be an algorithm or another model), $g$, such that $g(\mathbf{x})=\boldsymbol{\eta}$, producing the input perturbation $\boldsymbol{\eta}$, which produces an adversarial example:

\begin{equation}
    \mathbf{x'} = \mathbf{x} + \boldsymbol{\eta}
\end{equation}

such that $f(\mathbf{x'}) \ne f(\mathbf{x})$, for classifier $f$. To make such perturbations indistinguishable for humans, mathematically, the perturbations are constrained to be within a magnitude $\epsilon$ and are typically constrained using the L1 norm:

\begin{equation}
\label{eq:l1norm}
    ||\mathbf{x'} - \mathbf{x}||_1 = \sum_{i=1}^k |x_i - x_i'| < \epsilon
\end{equation}

the L2 norm:

\begin{equation}
\label{eq:l2norm}
    ||\mathbf{x'} - \mathbf{x}||_2 = \sum_{i=1}^k (x_i - x_i')^2 < \epsilon
\end{equation}

the $\text{L}_\infty$ norm:

\begin{equation}
\label{eq:linfnorm}
    ||\mathbf{x'} - \mathbf{x}||_\infty = \max_i |x_i-x_i'| < \epsilon
\end{equation}

for some small $\epsilon$.

\subsubsection{FGSM}
\label{sec:fgsm_attack}

The Fast Gradient Sign Method (FGSM) was introduced by Goodfellow in \cite{goodfellow2015}. The attack takes the model parameters $\boldsymbol{\theta}$, the input $\mathbf{x}$, and the target $y$ and computes the gradient of the loss W.R.T. the input $\mathbf{x}$ to obtain an adversarial perturbation:

\begin{equation}
    \boldsymbol{\eta} = \epsilon\times \text{sign}(\nabla_{\boldsymbol{x}} J(\boldsymbol{\theta}, \mathbf{x}, y))
\end{equation}

Where $\epsilon$ is some small value. This perturbation is then added to the input to obtain the adversarial example $\mathbf{x'}=\mathbf{x} + \boldsymbol{\eta}$. Note that $\mathbf{x'}$ is typically clipped so that its feature values do not exceed the boundaries of the data; i.e., clipped to be within [0, 1] for [0, 1]-scaled images.

\subsubsection{BIM}

The Basic Iterative Method (BIM) was introduced by Kurakin in \cite{bim_paper} and extends the attack in section \ref{sec:fgsm_attack} with another parameter $\alpha$ to make the attack iterative. The number of iterations is typically computed as $T=\epsilon / \alpha$. The attack begins with initialising the adversarial example to $\mathbf{x'}_0=\mathbf{x}$, and then iteratively computes:

\begin{equation}
\mathbf{x'}_{t+1} = \text{clip}\left( 
    \mathbf{x'}_t + \alpha \times \text{sign}(\nabla_{\boldsymbol{x}} J(\boldsymbol{\theta}, \mathbf{x'}_t, y)) 
    \right)
\end{equation}

Where clip is the function described in section \ref{sec:fgsm_attack} to ensure that the example remains inside the valid input space. 

\subsubsection{DeepFool}



\subsection{Identification Attempts}

\subsection{Robustness and Uncertainty}

How robustness is achieved and how uncertainty quantification fits into this. THIS IS PROBABLY BEST PUT INTO THE PREVIOUS WORK SECTION.


The distribution of logits may also vary depending on the examples. Intuitively, since the purpose of an adversarial attack is to move examples across the decision boundary, the examples should end up crossing the boundary or at least get closer to it. Since a decision boundary is where the logits of a model are equal, at least between the two classes on both sides of the boundary, this means that adversarial attacks generally push examples towards a higher loss. Looking at Equations \ref{eq:crossentropyloss} and \ref{eq:entropy}, we see that the cross entropy function is very similar to the entropy, except that the cross entropy function takes only the correct class' log-probability and the entropy takes a predicted-probability weighted sum of the log-probabilities. Eq. \ref{eq:crossentropyloss} is small when the predicted probability for the correct class is close to 1 and Eq. \ref{eq:entropy} is small when one class dominates the logits. This means if cross entropy is the loss used, there is an inherent relationship between the loss and the entropy function. 

By this rational, the location of adversarial examples in the feature space indicates regions where the loss and entropy are both high, and due to the proximity of the decision boundary, the model's predictions should not be confident. 


\section{Abstention}
\label{sec:abstention}

* selective classification, reject option classifiers, confidence calibration, abstention using c


\section{Summary}

Why your work fills the gaps
* no research uses applicability domain elsewhere
* uncertainty is used to optimise adversarial attacks but not to catch them
* etc


\chapter{Previous Work}

\section{taken from entropy background}

Previous literature has explored the use of these logits for confidence calibration to align the predicted probabilities (i.e. softmax over the logits) with the true likelihood of correctness \cite{confidence_calibration}. 

The work in \cite{logit_dist_adv_trained_dnn} examined the logit-distributions of adversarially-trained deep neural networks (DNN), finding that standard DNNs have a more spread distribution of ``logit gaps'' (difference between the highest and second highest logits) and that adversarial training \cite{goodfellow2015} (training DNN on standard and adversarial examples) increases these logit gaps \cite{logit_dist_adv_trained_dnn}; i.e., the distribution of the logit gaps becomes less spread, with lower mean, with a stronger positive skew, leading to less confident predictions and more effort by the model to learn the true underlying features. The implication is that, since the logit gap is smaller, if the logits become more uniform then the entropy of the predictions should be higher. 





\chapter{Methodology Motivation}

The aim of this section is to provide a motivation behind the methodology and explaining the rationale behind the design decisions. This section focuses only on the simulated chessboard/checkerboard dataset described in Section \ref{sec:chessboard_dataset} and the model used is described ion Section \ref{sec:chessboard_model}. This is a simulated dataset, where the training and testing datasets come from the exact same distribution. The dataset is relatively hard for a model to learn due to the diagonal decision boundary lines the classifier is required to learn, because the classifier is forced to learn non-linear boundaries.


\section{Decision Boundaries}

\begin{wrapfigure}{r}{0.5\textwidth}
    \vspace{-10pt}
    \centering
    \includegraphics[width=0.5\textwidth]{images/chessboard_training.png}
    \caption{Training and test accuracy by epoch trained on the chessboard dataset.}
    \label{fig:chessboard_training}
    \vspace{-0pt}
\end{wrapfigure}

Figure \ref{fig:chessboard_training} shows the training and test accuracies by epoch trained on the chessboard dataset. The vertical horizontal line, at epoch 30, shows the first occurrence of where the training starts to plateau. After this point, the model's accuracy on both the training and test sets oscillates slightly, although roughly remains constant. It is important to note that the model's test performance is expected to be almost identical for the training and test sets due to the construction of this dataset.


Figure \ref{fig:chessboard_training_boundaries} shows the decision boundaries learned for every 10th epoch, the same epochs as in Figure \ref{fig:chessboard_training}. The title for each facet shows the epochs trained and the corresponding test accuracy. 

Figure \ref{fig:chessboard_training_boundaries} shows that effective decision boundaries start being learned at epoch 30, which continues up to and including epoch 50 and then starts overfitting, which is consistent with the vertical line in Figure \ref{fig:chessboard_training}. Here, it is easy to tell that the model is overfitting because instead of learning diagonal decision boundaries, the model starts wrapping some clusters inside their own decision boundary; this behaviour is most evident for epoch 100. However, with just the training/test accuracy, seen in Figure \ref{fig:chessboard_training}, understanding the shape of the boundary is much harder.


\begin{figure}[h!]
    \centering
    \includegraphics[width=1.0\textwidth]{images/chessboard_training_boundaries.png}
    \caption{Decision boundaries for every 10 epochs trained. Note that the test points are plotted for visibility.}
    \label{fig:chessboard_training_boundaries}
\end{figure}

A point on the decision boundary, naturally, represents the most uncertainty as the model does not know which class to assign the prediction to. For points that are very close but not quite on the boundary, the model's uncertainty should still be high, since the decision boundary learnt by the model is never perfect; data is noisy, the data-generating process may be faulty, and the model is just an approximation of the true underlying relationships. In the areas around a decision boundary, even a slight change to the input features may cause the class label to be flipped. Although this simulated example is in two dimensions, higher-dimensional spaces have the same properties, albeit with much more complicated decision boundaries that are impossible to visualise. 

\section{Subtelties - possibly move to discussion}
\label{sec:subtleties}

One subtlety, which is not illustrated in these plots, is that the decision boundaries can be more complex when there are more than two classes. This is perhaps best illustrated using predicted probabilities with an example using four classes. 

If the two highest predicted probabilities are the same, then the point lies on the boundary between two of the four classes. If the highest three  predicted probabilities are the same, the point lies at the intersection of the decision boundaries of three classes. Finally, if all of the predicted probabilities are the same, then the point lies at the intersection of all four of decision boundaries. This is relevant, because entropy (Eq. \ref{eq:entropy}) and information content (Eq. \ref{eq:information_content}) differ in how they approach these scenarios. 

Entropy measures the "flatness" of the predicted-probability distribution - if the predicted probabilities are all the same, then entropy is maximised. However, if the two highest predicted probabilities are the same, say for classes $a$ and $b$, then entropy can be increased if any of the other logits corresponding to, say, class $c$ are increased. Since predicted probabilities are just normalised logits, it makes more sense to continue with logits rather than probabilities. Nevertheless, increasing $c$'s logit translates to moving along the decision boundary between class $a$ and $b$ in a specific direction; towards the intersection of the boundaries of classes $a$, $b$, and $c$, where all logits (and predicted probabilities) will become equal. Therefore higher entropy values do not necessarily correspond to decision boundaries when there is more than two classes. If there are 1000 classes, as in ImageNet, then high entropies are the areas close to where a large number of decision boundaries meet, rather than areas between two decision boundaries. 

On the other hand, the information content (Eq. \ref{eq:information_content}) relies only on the maximum predicted probability, ignoring all others. This means the information content for $\mathbf{p}=[0.4, 0.4, 0.2]$ and $\mathbf{p}=[0.4, 0.3, 0.3]$ is identical. This is problematic, because in the former, the input point lies on the decision boundary (where there should be more uncertainty) than in the latter, where the point lies within the boundary of class $a$ and is equidistance from classes $b$ and $c$. Information content cannot distinguish between these two cases. Because of this issue, it is natural to consider the logit-gaps (briefly discussed in \ref{sec:background_uncertainty_entropy}), since they do not exhibit these problems. 

\section{Loss}

Machine learning models learn by minimising the loss, typically the cross-entropy loss defined in Eq. \ref{eq:crossentropyloss}, which is able to act as a functional proxy for the decision boundaries. Unless the learning is weighted, by assigning more weight to certain classes for example, Eq. \ref{eq:crossentropyloss} is averaged and this value is minimised. However, without this reduction, loss is calculated per point and therefore by calculating this loss for every possible point within the range of the training data, it is possible to plot the loss landscape. Figure \ref{fig:chessboard_loss} shows this loss landscape as the model trains. Note that Figure \ref{fig:chessboard_loss} is an identical plot to \ref{fig:chessboard_training_boundaries} except that the contours are showing loss rather than decision boundaries. 

\begin{figure}[h!]
    \centering
    \includegraphics[width=1.0\textwidth]{images/chessboard_loss.png}
    \caption{Loss landscape for every 10 epochs trained. Note that the test points are plotted for visibility.}
    \label{fig:chessboard_loss}
\end{figure}

Figure \ref{fig:chessboard_loss} shows that the loss for most points starts off being high everywhere and, through training, decreases on average, although the loss remains relatively high in the regions where the decision boundaries are. Note that no matter how overfit the model gets (epochs > 30), the location around the decision boundary have high loss. This means that if the loss is high, the model is uncertain, defining the decision boundaries. However, as discussed in Section \ref{sec:background_uncertainty_entropy}, the problem is that to calculate the loss, the target labels are required, and one must resort to approximations.




\section{Approximations}

Two alternative approaches are entropy defined in Eq. \ref{eq:entropy} and information gain defined in Eq. \ref{eq:information_content}. 

The following figure follows on from the Fig. \ref{fig:chessboard_four_classes_boundary} example and shows the loss, entropy, information content, and probability gaps for the four-class classification problem.

\begin{figure}[h!]
    \centering
    \includegraphics[width=1.0\textwidth]{images/chessboard_four_classes_comparison.png}
    \caption{Entropy and Information Content for every 10 epochs trained. Note that the test points are plotted for visibility.}
    \label{fig:chessboard_loss}
\end{figure}

\section{Adversarial Examples}






\chapter{Methodology}








\section{Theoretical Foundation}

\subsection{Uncertainty Measures}

Define entropy, information content, and other uncertainty measures used.

\subsubsection{Entropy}





\subsubsection{Applicability Domain}

Formalise the applicability domain framework for ML models. 

* What does it mean (for the purposes of this work to accept or reject a predcition?)

\subsection{Proposed Approaches}

\subsubsection{Abstention Strategy}

* Uncertainty-based Abstention Strategy using entropy and conformal prediction guarantees

\subsubsection{Identification Strategy}

* Uncertainty-based Identification Strategy

\subsubsection{Applicability domain}

* how the previous two subsubsections fit into the applicability domain framework.


\section{Experimental Setup}

This section is broken up into multiple parts. First, there is a discussion on the datasets used, followed by an explanation of models and their architecture. Then, THERE IS THE DISTANCE SECTION FOLLOWED BY THE ADVERSARIAL REGION SECTION.

This work uses Scikit-Learn (REFERENCE), PyTorch \cite{pytorch} and the datasets described below. Since this work is interested in exploring model quality rather than tuning hyperparameters, it is deemed that typical model selection strategies such as cross-validation or a separate validation set are unnecessary, since all experiments make use only of the training set. 

All models are trained on the training set and adversarial examples generated from the training set only. The models nor attacks see the test set directly nor indirectly. However, models are evaluated for their performance using the test set and whether a model is overfit is determined by the test set.

The results in Chapter \ref{cha:results} were obtained using an M4 Max GPU. THE CODE IS AVAILABLE HERE.


\subsection{Datasets}

Please note that unless otherwise specified, all datasets are scaled using [0, 1]-scaling with the following formula:

\begin{equation}
    X_j = \frac{X_j - \text{max}(X_j)}{\text{max}(X_j) - \text{min}(X_j)}
\end{equation}

for feature $j$.

\subsubsection{Chessboard}
\label{sec:chessboard_dataset}

\subsubsection{MNIST}

MNIST is a famous dataset of 70,000 grayscale examples of handwritten digits. Containing digits from 0 to 9 with each image being of size 28 $\times$ 28, width and height respectively. Since the images are grayscale, there is no colour and consequently there is only one colour dimension. The dataset is pre-split into 60,000 training points and 10,000 test points. Some examples can be seen in the top row of Fig. \ref{fig:adversarial_attacks}.

The MNIST dataset is popular because of its simplicity, balanced class distribution, and ease of training. It is quite trivial to obtain a test accuracy of 98\%+ on MNIST.

No pre-processing was done except convert the PIL images to tensors for use with PyTorch. MNIST images are already normalised with pixel values within [0, 1]. 


\subsubsection{SpamBase}

The SpamBase dataset \cite{spambase_94} is a spam classification task. There are 4,140 examples in total, with 1,630 spam and 2,510 non-spam. There are 57 numeric features and all are engineered variables. Some examples of the features are the character frequencies of certain symbols (such as !, \$, and \#), words, or longest running sequence of capital letters etc. See \cite{spambase_94} for a full description.

For preprocessing, Scikit-Learn is used to make training and testing splits, the data is min-max scaled so that all columns are within [0, 1]. The scaler is fit on the training set and the test set is transformed using the learned parameters.

In some experiments, a covariate shift (see \ref{sec:distribution_shifts}) is induced. The process for this is as follows: $k$ features are selected for inducing a covariate shift ($k=57$ unless otherwise specified). Each feature is randomly allocated to have an additive or multiplicative shift applied. For an additive shift, the product of an "intensity" parameter and each features standard deviation is applied. For a multiplicative shift, each column is multiplied by "intensity" plus 1. The effect of a covariate shift can be seen in Fig. \ref{fig:induced_covariate_shift}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{images/induced_covariate_shift_spambase.png}
    \caption{UMAP Projection of an induced covariate shift on the SpamBase dataset.}
    \label{fig:induced_covariate_shift}
\end{figure}

\subsubsection{Heloc}

\subsubsection{ImageNet}


\subsection{Models}

\subsubsection{Chessboard}
\label{sec:chessboard_model}



\subsubsection{SpamBase}

All SpamBase models had the same architecture. The models differed in the amount of training done and the parameters used.

The architecture for all models was the following. The network had three linear layers with weights tensors of size 57x128, 128x32, and 32x2. The linear layers were separated by ReLU and the cross entropy loss was used. Adam was used as an optimizer with all default parameters. A learning rate of 0.01 was used.

Two models were trained and the training cutoffs were determined using Fig. \ref{fig:spambase_train_test}. The standard model was trained until 60 epochs and the overfit was trained until 400 epochs. Since this paper is not about training the best possible model, it is logical that validating using the test set directly was enough and cross validation was not required to decide on a model, since we are exploring the properties of overfit models. The results are summarised in Table \ref{tab:spambase_training}.

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
         & Normal & Overfit \\ 
         \hline
        Epochs & 60 & 400 \\ 
        \hline \hline
        Train & 98.86\% & 99.59\% \\ \hline
        Test & 95.23\% & 94.58\% \\ \hline
    \end{tabular}
    \caption{Model training results on the SpamBase dataset.}
    \label{tab:spambase_training}
\end{table}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{images/spambase_train_test.png}
    \caption{Training and Test Accuracy by Epoch for network trained on the SpamBase dataset. The dashed blue vertical lines show where the normal and overfit models were trained up to.}
    \label{fig:spambase_train_test}
\end{figure}












\chapter{Results}
\label{cha:results}






\section{Model Performance}

Table \ref{tab:model_performance} shows the performance of models across the four main datasets investigated in this work. 
We observe that the training and testing accuracies are generally very similar for all models, with the largest discrepancy for the chessboard dataset. However, since this model was trained for 50 epochs, and the dataset was simulated specifically to be difficult for the model to learn effective boundaries, the model is most likely slightly overfit to the training data. Note that for the ImageNet dataset, this work uses the pre-trained ResNet-50 developed in \cite{he2015deepresiduallearningimage}.

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
        \hline
         & Chessboard & SpamBase & MNIST & ImageNet \\ 
         \hline
        Epochs & 30 & 6 & 5 & - \\
        \hline \hline
        Train & 87.58\% & 94.66\% & 99.6\% & - \\ \hline
        Test & 90.25\% & 94.58\% & 98.94\% & 78.85\% \\ \hline
    \end{tabular}
    \caption{Model performance results by dataset.}
    \label{tab:model_performance}
\end{table}

\section{Abstention for Accuracy Improvement}

* apply abstention with entropy, information content etc.
* coverage vs. accuracy
* compare against a baseline - confidence thresholding


\section{Adversarial Identification}

* ROC, AUC, precision-recall metrics
* compare with existing adversarial detection methods

\section{Applicability Domain Integration}








\chapter{Discussion}

* strengths and limitations of uncertainty measures
    - doesn't work as well on imagenet?
* when abstention helps most
* when the adversarial detection works (and fails)
* how applicability domains shift the reliability story

* compare against related work
* theoretical implications

\chapter{Conclusions}




		
			
% --------------------------------------------------------------

% --------------------------------------------------------------
\renewcommand{\bibname}{References} % changes the header; default: Bibliography
% \bibliographystyle{plainnat}
\bibliographystyle{unsrtnat}
\bibliography{Bibliography}


\appendix%%% start appendices here 




\end{document}
