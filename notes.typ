= Notes

== Goals
- Develop an adversarial framework (with metric)
- For testing how good a model is, specifically out-of-distribution
- Find a boundary to test the applicability domain

== What I need to do
- Define the applicability domain - defined as an indicator for when a predictor acts inside the domain it works reliably on. (applicability, reliability, decidability)
- Figure out how to find the applicability domain


== Stuff to research
+ when test distribution is different from train
+ Applicability domain
+ finding applicability domains with adversarial learning




=== Questions for joerg

+ How does my project differ from the BAARD paper? Thoughts:

1. baard proposed a framework to detect adversarial attacks. My project would be using adversarial learning directly to find and isolate the domain?
2. and to develop a metric/methodology to evaluate a model's performance based on the domain.

look into bias

overfitting
adversarial regions

